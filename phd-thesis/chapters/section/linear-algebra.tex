\section{Linear algebra} % (fold)
\label{sec:linear_algebra}

Quantum computation requires familiarity with the basics of linear algebra. This section will give
definitions of the terms used throughout this thesis.
\subsection{Basic definitions} % (fold)
\label{sub:basic_definitions}


We start with the definition of a \emph{vector space}.

\begin{definition}\label{def:vector_space}
  Given a field $F$, whose elements will be referred to as scalars, a \emph{vector space} over $F$
  is a non-empty set $V$ with two operations, \emph{vector addition} and \emph{scalar
  multiplication}. \emph{Vector addition} is denoted by an infix $+$ and the set $V$ must be an
  abelian group under $+$.
  \emph{Scalar multiplication} is a function $F\times V \to V$ and denoted by juxtaposing the scalar
  from $F$ and the vector from $V$, i.e., $c\vc{v}$. Scalar multiplication distributes over both
  vector  addition and addition of scalars.  Scalar multiplication is associative. $F$'s multiplicative identity is an
  identity for scalar  multiplication.
\end{definition}
The specific algebraic requirements are:
\begin{enumerate}
  \item{}$\forall \vc{u},\vc{v},\vc{w} \in V,\ (\vc{u} +\vc{v}) +\vc{w} =
    \vc{u}+ (\vc{v}+\vc{w})$;
  \item{}$\forall \vc{u},\vc{v} \in V,\ \vc{u} +\vc{v} =
    \vc{v}+ \vc{u}$;
  \item{}$\exists  \vc{0} \in V \mathrm{\ such\ that\ } \forall \vc{v} \in V,
    \vc{0} +\vc{v} =  \vc{v}$;
  \item{}$\forall \vc{u} \in V, \exists \vc{v} \in V \mathrm{\ such\ that\ }
    \vc{u}+ \vc{v} = \vc{0}$;
  \item{}$\forall \vc{u},\vc{v} \in V, c\in F,\
    c(\vc{u}+ \vc{v}) = c\vc{u} + c\vc{v}$;
  \item{}$\forall \vc{u} \in V, c,d\in F,\
    (c+d)\vc{u} = c\vc{u} + d\vc{u}$;
  \item{}$\forall \vc{u} \in V, c,d\in F,\
    (c d)\vc{u} = c(d\vc{u})$;
  \item{}$\forall \vc{u} \in V,\
    1\vc{u} = \vc{u}$.
\end{enumerate}

Examples of vector spaces over $F$ are:
\begin{itemize}
  \item $F^{n\times m}$ -- the set of $n\times m$ matrices over $F$;
  \item $F^n$ -- the $n{-}$fold Cartesian product of $F$.
\end{itemize}

$F^{n\times 1}$, the set of $n\times 1$
matrices over $F$ is also called the space of column vectors, while $F^{1\times n}$ is called the
space of row vectors. As is customary, we shall identify $F^n$ with $F^{n\times 1}$.

\begin{definition}\label{def:linearly_independent}
  A set of vectors $\{\vc{v}_i\}$ in the vector space $V$ is said to be \emph{linearly
  independent} when no finite linear combination of them, $\sum a_j\vc{v}_j$, equals \vc{0} unless
  all the $a_j$ are zero.

\end{definition}

\begin{definition}\label{def:basis}
  A \emph{basis} of a vector space $V$ is a linearly independent subset of $V$ that generates $V$.
  That is, any vector $u \in V$ is a linear combination of the basis vectors.
\end{definition}

\begin{definition}\label{def:linear_map_of_vector_spaces}
  Suppose we have $V, W$ vector spaces over $F$ with $v \in V$ and $s \in F$. We say $f$ is a
  \emph{linear map} when $f:V \to W$ is a group homomorphism such that $f(v s) = f(v)s$. Let
  $f_v$ be the
  map obtained from $f$ by fixing $v\in V$ and $f_w$ be the map obtained from $f$ by fixing  $w\in W$.
  Then, a map $f:V\times W \to X$ is called \emph{bilinear} when the
  map $f_v:W\to T$ and $f_w:V\to T$ are linear for each $v\in V$ and $w\in W$, where
\end{definition}

\begin{definition}\label{def:free_vector_space}
  Given a set $S$, the \emph{free vector space} of $S$ over a field $F$ is the abelian group
  of formal sums $\sum a_i s_i$ where the $s_i$ are the elements of $S$ and $a_i \in F$.
  Formal sums are independent of order. Addition is defined as $(\sum a_i s_i) + (\sum b_i s_i)$ is
  $(\sum (a_i + b_i) s_i)$.
\end{definition}

\begin{definition}\label{def:tensor_product_of_vector_spaces}
  Given vector spaces $V, W$ over the base field $F$, consider the free vector space of $V
  \times W = F(V\times W)$. Next, consider the subspace $T$ of $F(V\times W)$ generated by the
  following equations:
  \begin{align*}
    (v_1,w)+(v_2,w) & = (v_1+v_2,w)\\
    (v,w_1)+(v,w_2) & = (v, w_1+w_2)\\
    s(v,w) &= (s v,w)\\
    s(v,w) &= (v,s w),
  \end{align*}
  where $v,v_1,v_2 \in V$, $w,w_1,w_2 \in W$ and $s\in F$. Then the tensor product of
  $V$ and $W$, written $V\*W$ is $F(V\times W)/T$.
\end{definition}

Elements of the tensor product $V\*W$ are written as $v\*w$ and are the $T$-equivalence class of
$(v,w) \in V\times W$. If $\{v_i\}$ is a basis for $V$ and $\{w_j\}$ is a basis for $W$, then the
elements $\{v_i\*w_j\}$ form a basis for $V\*W$.
% subsection basic_definitions (end)
\subsection{Matrices} % (fold)
\label{sub:matrices}


As mentioned above, the set of $n\times m$ matrices over a field is a vector space. Additionally,
matrices compose and the tensor product of matrices is defined.

Matrix composition is defined as usual. That is, for $A = [a_{i j}] \in F^{m\times n}, B =
[b_{j k}]\in F^{n \times p}$:
  \[
    A \, B = \left[\left(\sum_{j}a_{i j}b_{j k}\right)_{i k}\right] \in F^{m \times p}.
  \]



\begin{definition}\label{def:diagonal_matrix}
  A \emph{diagonal matrix} is a matrix where the only non-zero entries are those where the column
  index equals the row index.
\end{definition}

The diagonal matrix $n\times n$ with only $1$'s on the diagonal is the identity for matrix
multiplication, and is designated by $I_n$.

\begin{definition}\label{def:transpose}
  The \emph{transpose} of a $n\times m$ matrix, $A=[a_{i j}]$, is a $m\times n$ matrix $A^{t}$ with
  the $i,j$ entry being $a_{j i}$.
\end{definition}

\begin{definition}\label{def:conjugate_transpose}
  When the base field of a matrix is \complex, the complex numbers, the \emph{conjugate transpose}
  of an $n\times m$ matrix $A=[a_{i j}]$ is defined as the $m\times
  n$ matrix $A^{*}$ with the $i,j$ entry being $\conjugate{a}_{j i}$. The conjugate transpose is
  also referred to as the \emph{adjoint} of the matrix.
\end{definition}

When working with column vectors over \complex, note that $\vc{u} \in \complex^n \implies
\vc{u}^{*} \in \complex^{1\times n}$ and that $\vc{u}^{*}\times \vc{u} \in \complex^{1\times 1}$.
This thesis will use the usual identification of \complex{} with $\complex^{1\times1}$.

\begin{definition}\label{def:unit_vector}
  If \vc{u} is a \complex column vector, \vc{u} is called a \emph{unit vector} when
  $\vc{u}^{*}\times \vc{u} = 1$.
\end{definition}

\begin{definition}\label{def:trace}
  The \emph{trace}, $Tr(A)$, of a square matrix $A=[a_{i j}]$ is $\sum a_{ii}$.
\end{definition}

The tensor product of two matrices is the usual Kronecker product:
  \[
    U\otimes V =
    \begin{bmatrix}
      u_{11}V&u_{12}V & \cdots &u_{1m}V\\
      u_{21}V&u_{22}V & \cdots &u_{2m}V \\
      \vdots&\vdots&\ddots\\
      u_{n1}V&u_{n2}V & \cdots &u_{n m}V
    \end{bmatrix}
    =
    \begin{bmatrix}
      u_{11}v_{11}&\cdots&u_{12}v_{11} & \cdots& u_{1m}v_{1q} \\
      u_{11}v_{21}&\cdots&u_{12}v_{21} & \cdots& u_{1m}v_{2q} \\
      \vdots&\vdots&\vdots&\ddots \\
      u_{n1}v_{p1}&\cdots&u_{n2}v_{p1} & \cdots& u_{n m}v_{p q} \\
    \end{bmatrix}
  \]
% subsubsection tensor_product (end)

\subsubsection{Special matrices} % (fold)
\label{ssub:special_matrices}

When working with quantum values certain types of matrices over the complex numbers are of special
interest. These are:
\begin{description}
  \item[Unitary Matrix]: Any $n \times n$  matrix $A$ with $A A^{*} = I\ (= A^{*} A)$.
  \item[Hermitian Matrix]: Any  $n \times n$ matrix $A$ with $A=A^{*}$.
  \item[Positive Matrix]: Any Hermitian matrix $A$ in  $\complex^{n\times n}$
    where $\vc{u}^{*} A \vc{u} \ge 0$ for all vectors  $\vc{u}\in \complex^n$. Note
    that for any Hermitian matrix $A$ and vector $u$,  $\vc{u}^{*} A \vc{u}$ is real.
  \item[Completely Positive Matrix]: Any positive matrix $A$ in  $\complex^{n\times n}$
    where $I_m \otimes A$ is positive.
\end{description}
The matrix
  \[
    {\begin{singlespace}
      \begin{bmatrix}
        0&-i\\
        i&0
      \end{bmatrix}
    \end{singlespace}}
  \]
is an example of a matrix that is \emph{unitary}, \emph{Hermitian}, \emph{positive} and
\emph{completely positive}.


% subsubsection special_matrices (end)

\subsubsection{Superoperators} % (fold)
\label{ssub:superoperators}

A \emph{Superoperator} $S$ is a matrix over \complex{} with the following restrictions:
\begin{enumerate}
  \item{} $S$ is \emph{completely positive}. This implies that $S$ is positive as well.
  \item{} For all positive matrices $A$, $Tr(S\,A) \leq Tr(A)$.
\end{enumerate}
% subsubsection superoperators (end)
% subsection matrices (end)

% section linear_algebra (end)

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../phd-thesis"
%%% End:
