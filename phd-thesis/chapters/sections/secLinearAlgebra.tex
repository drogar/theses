%!TEX root = /Users/gilesb/UofC/thesis/phd-thesis/phd-thesis.tex
\section{Linear algebra}\label{sec:linearalgebra}
Quantum computation requires  familiarity with
the basics of linear algebra. This section will give definitions of the
terms used throughout this thesis.
\subsection{Basic definitions}\label{subsec:labasicdefinitions}

The first definition needed is that of a \emph{vector space}.

\begin{definition}[Vector Space]
Given a field $F$, whose elements will be referred to as scalars,
 a \emph{vector space} over $F$ is a non-empty set $V$ with
two operations, \emph{vector addition} and \emph{scalar multiplication}.
\emph{Vector addition} is defined
as ${+}:V\times V \to V$ and denoted as $\vc{v}+\vc{w}$
 where
$\vc{v},\vc{w}\in V$. The set $V$ must be an abelian group under $+$.
\emph{Scalar multiplication} is defined as ${}:F\times V \to V$ and denoted as
$c\vc{v}$ where $c\in F, \vc{v} \in V$. Scalar multiplication
distributes over both vector addition and scalar addition and is
 associative. $F$'s multiplicative identity is
an identity for scalar multiplication.
\end{definition}
The specific algebraic requirements are:
\begin{enumerate}
\item{}$\forall \vc{u},\vc{v},\vc{w} \in V,\ (\vc{u} +\vc{v}) +\vc{w} =
 \vc{u}+ (\vc{v}+\vc{w})$;
\item{}$\forall \vc{u},\vc{v} \in V,\ \vc{u} +\vc{v} =
 \vc{v}+ \vc{u}$;
\item{}$\exists  \vc{0} \in V \mathrm{\ such\ that\ } \forall \vc{v} \in V,
 \vc{0} +\vc{v} =  \vc{v}$;
\item{}$\forall \vc{u} \in V, \exists \vc{v} \in V \mathrm{\ such\ that\ }
 \vc{u}+ \vc{v} = \vc{0}$;
\item{}$\forall \vc{u},\vc{v} \in V, c\in F,\
 c(\vc{u}+ \vc{v}) = c\vc{u} + c\vc{v}$;
\item{}$\forall \vc{u} \in V, c,d\in F,\
 (c+d)\vc{u} = c\vc{u} + d\vc{u}$;
\item{}$\forall \vc{u} \in V, c,d\in F,\
 (c d)\vc{u} = c(d\vc{u})$;
\item{}$\forall \vc{u} \in V,\
 1\vc{u} = \vc{u}$.
\end{enumerate}

Examples of vector spaces over $F$ are: $F^{n\times m}$ -- the set
of $n\times m$ matrices over $F$; and $F^n$ --  the $n{-}$fold
Cartesian product of $F$.
$F^{n\times 1}$, the set of $n\times 1$ matrices
over $F$ is also called the space of column vectors, while
$F^{1\times n}$, the set of
row vectors.   Often,
$F^n$ is identified with $F^{n\times 1}$.

This thesis  shall identify $F^n$ with the column vector space over $F$.

\begin{definition}[Linearly independent]
A subset of vectors $\{\vc{v}_i\}$ of the vector space $V$ is said to be
\emph{linearly independent} when no finite linear combination of them,
$\sum a_j\vc{v}_j$ equals \vc{0} unless all the $a_j$ are zero.
\end{definition}

\begin{definition}[Basis]
A \emph{basis} of a vector space $V$ is a linearly independent subset of
$V$ that generates $V$. That is, any vector $u \in V$ is a linear combination
of the basis vectors.
\end{definition}

\subsection{Matrices}\label{subsec:lamatrices}
As mentioned above, the set of
 $n\times m$ matrices over a field is a vector space.
Additionally, matrices compose and the tensor product of matrices is defined.

Matrix composition is defined as usual.
That is,
for $A = [a_{ij}] \in F^{m\times n}, B = [b_{jk}]\in F^{n \times p}$:
\[A \, B = \left[\left(\sum_{j}a_{ij}b_{jk}\right)_{ik}\right]
\in F^{m \times p}.\]


\begin{definition}[Diagonal matrix]
A \emph{diagonal matrix} is a matrix where the only non-zero
entries are those where the column index equals the row index.
\end{definition}

The diagonal matrix  $n\times n$ with only $1$'s on the diagonal
is the identity for matrix
multiplication, and is designated by $I_n$.

\begin{definition}[Transpose]
The \emph{transpose} of an $n\times m$ matrix $A=[a_{ij}]$ is
an $m\times n$ matrix $A^{t}$ with the $i,j$ entry being $a_{ji}$.
\end{definition}

When the base field of a matrix is \complex, the complex numbers,
the \emph{conjugate transpose} (also called the \emph{adjoint})
of  an $n\times m$ matrix $A=[a_{ij}]$ is defined
as the $m\times n$ matrix
$A^{*}$ with
the $i,j$ entry being $\overline{a}_{ji}$,
where  $\overline{a}$ is the complex conjugate of $a\in\complex$.

When working with column vectors over \complex, note that
$\vc{u} \in \complex^n \implies \vc{u}^{*} \in
\complex^{1\times n}$ and that
 $\vc{u}^{*}\times \vc{u} \in \complex^{1\times 1}$.
This thesis will use the usual identification of
\complex{} with $\complex^{1\times1}$. A column
vector \vc{u} is called a \emph{unit vector}
when $\vc{u}^{*}\times \vc{u} = 1$.

\begin{definition}[Trace]
The \emph{trace}, $Tr(A)$ of a square matrix $A=[a_{ij}]$ is $\sum a_{ii}$.
\end{definition}



\subsubsection{Tensor product}
The tensor product of two matrices is the usual Kronecker product:
\[U\otimes V =
\begin{bmatrix}
u_{11}V&u_{12}V & \cdots &u_{1m}V\\
u_{21}V&u_{22}V & \cdots &u_{2m}V \\
\vdots&\vdots&\ddots\\
u_{n1}V&u_{n2}V & \cdots &u_{nm}V
\end{bmatrix}
=
\begin{bmatrix}
u_{11}v_{11}&\cdots&u_{12}v_{11} & \cdots& u_{1m}v_{1q} \\
u_{11}v_{21}&\cdots&u_{12}v_{21} & \cdots& u_{1m}v_{2q} \\
\vdots&\vdots&\vdots&\ddots \\
u_{n1}v_{p1}&\cdots&u_{n2}v_{p1} & \cdots& u_{nm}v_{pq} \\
\end{bmatrix}
\]

\subsubsection{Special matrices}
When working with quantum values certain types of matrices over the
complex numbers are of special interest.
These are:
\begin{description}
\item[Unitary Matrix]: Any $n \times n$
matrix $A$ with $A A^{*} = I\ (= A^{*} A)$.
\item[Hermitian Matrix]: Any  $n \times n$ matrix $A$ with $A=A^{*}$.
\item[Positive Matrix]: Any Hermitian matrix $A$ in
$\complex^{n\times n}$
where $\vc{u}^{*} A \vc{u} \ge 0$ for all vectors
$\vc{u}\in \complex^n$. Note
that for any Hermitian matrix $A$ and vector $u$,
$\vc{u}^{*} A \vc{u}$ is real.
\item[Completely Positive Matrix]: Any positive matrix $A$ in
$\complex^{n\times n}$
where $I_m \otimes A$ is positive.
\end{description}
The matrix
\[{\begin{singlespace}\begin{bmatrix}0&-i\\
i&0\end{bmatrix}\end{singlespace}}\]
is an example of a matrix that is \emph{unitary}, \emph{Hermitian},
\emph{positive} and \emph{completely positive}.

\subsubsection{Superoperators}
A \emph{Superoperator} $S$ is a matrix over \complex{}
with the following restrictions:
\begin{enumerate}
\item{} $S$ is \emph{completely positive}. This implies that
$S$ is positive as well.
\item{} For all positive matrices $A$, $Tr(S\,A) \leq Tr(A)$.
\end{enumerate}
